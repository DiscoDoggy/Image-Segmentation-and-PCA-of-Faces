# -*- coding: utf-8 -*-
"""fall2022_hw4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19wkwEVqTMQwHGqyPnEs1nw9GNQthcN6E

# CS171-EE142 - Fall 2022 - Homework 4

# Due: Friday, December 2, 2022 @ 11:59pm 

### Maximum points: 45 pts


## Submit your solution to Gradescope:
1. Submit a single PDF to **HW4**
2. Submit your jupyter notebook to **HW4-code**

**See the additional submission instructions at the end of this notebook**

### Enter your information below:

<div style="color: #000000;background-color: #EEEEFF">
    Your Name (submitter): Alexander Kaattari-Lim  <br>
    Your student ID (submitter): 862161616
    
<b>By submitting this notebook, I assert that the work below is my own work, completed for this course.  Except where explicitly cited, none of the portions of this notebook are duplicated from anyone else's work or my own previous work.</b>
</div>

## Academic Integrity
Each assignment should be done  individually. You may discuss general approaches with other students in the class, and ask questions to the TAs, but  you must only submit work that is yours . If you receive help by any external sources (other than the TA and the instructor), you must properly credit those sources, and if the help is significant, the appropriate grade reduction will be applied. If you fail to do so, the instructor and the TAs are obligated to take the appropriate actions outlined at http://conduct.ucr.edu/policies/academicintegrity.html . Please read carefully the UCR academic integrity policies included in the link.

# Overview 
In this assignment we will implement and test K-means algorithm for clustering and principal component analysis (PCA) for dimensionality reduction.  

If you are asked to **implement** a particular functionality, you should **not** use an existing implementation from the libraries above (or some other library that you may find). When in doubt, please ask. 

Before you start, make sure you have installed all those packages in your local Jupyter instance

## Read *all* cells carefully and answer all parts (both text and missing code)

You will complete all the code marked `TODO` and answer descriptive/derivation questions
"""

import numpy as np
import matplotlib.pyplot as plt
import random

# make sure you import here everything else you may need

"""## Question 1. K-Means Clustering [Total: 30 pts]

In this exercise we will first implement K-means algorithm for clustering. Then we will perform color-based segmentation using K-means.

## K-means clustering implementation [10 pts]

Let us first implement K-means algorithm that accepts target number of clusters (K) and data vectors $\mathbf{x}_1,\ldots, \mathbf{x}_N$, each of length $d$. At this point, we will implement the K-means algorithm for general $d$; later we will test and visualize the results for $d=2,3$. 
			
A general K-means algorithm can be described as follows. 
Suppose we are given training examples $\mathbf{x}_1, \mathbf{x}_2, \ldots, \mathbf{x}_N$, where each $\mathbf{x}_i \in \mathbb{R}^d$. We want to group the $N$ data samples into K clusters.  
* Initialize cluster centers $\mu_1,\ldots, \mu_K \in \mathbb{R}^d$ at random
* Repeat until convergence 
  * For every data point $\mathbf{x}_i$, update its label as 
  \begin{equation*}
  z_i = \text{argmin}_j \|\mathbf{x}_i - \mu_j\|_2^2. 
  \end{equation*}
  
  * For each cluster $j$, update its center $\mu_j$ as mean of all points assigned to cluster $j$: 
  $$ \mu_j = \dfrac{\sum_{i=1}^N \delta\{z_i = j\} \mathbf{x}_i}{\sum_{i=1}^N \delta \{z_i = j\}}.$$  
  $\delta\{z_i = j\}$ denotes an indicator function that is equal to 1 if $z_i = j$ and zero otherwise. $\sum_{i=1}^N\delta\{z_i = j\}$ indicates the number of points in $i$th cluster. 

We can define sum of squared errors (SSE) as 
$$ \text{SSE}  = \sum_j \sum_i \delta\{z_i = j\} \|\mathbf{x}_i - \mu_j\|_2^2$$`

Implement the K-means clustering algorithm as a function with the following specifications:

```python
def kmeans_clustering(data, K, max_iter = 100, tol = pow(10,-3)):
```
where 
1. 'data' is the $N \times d$ matrix that contains all data points ($N$ is the number of data points and $d$ is the number of features, each row of the matrix is a data point), 
2. 'K' is the number of clusters, 
3. 'max_iter' is the maxium number of iterations, and 
4. 'tol' is the tolerance for the change of the sum of squares of errors that determines convergence.

Your function should return the following variables: 
1. 'labels': this is an $N\times 1$ vector (where $N$ is the number of data points) where the $i$-th position of that vector contains the cluster number that the $i$-th data point is assigned to, 
2. 'centroids': this is a $K\times d$ matrix, each row of which contains the centroid for every cluster, 
3.  'SSE_history': this is a vector that contains all the sum of squares of errors per iteration of the algorithm, 
4. 'iters': this is the number of iterations that the algorithm ran.

Here we are going to implement the simplest version of K-means, where the initial centroids are chosen entirely at random among all the data points.

Your algorithm should converge if 1) the maximum number of iterations is reached, or 2) if the SSE between two consecutive iterations does not change a lot. 

In order to check for the latter condition, you may use the following piece of code:
```python
if np.absolute(SSE_history[it] - SSE_history[it-1])/SSE_history[it-1] <= tol
```
"""

# TODO 
# K-means clustering
def kmeans_clustering(data, K, max_iter=100, tol = pow(10,-3)): 
  # Inputs
  #   data - N x d array
  #   K - number of clusters
  #   max_iter - maximum iterations for K-means
  #   tol - stopping parameter that checks relative change in sum of squared errors
  # 
  # Outputs: 
  #   labels - cluster assignment label for each data sample (N values)
  #   centroid - centroids of each cluster (K vectors)
  #   SSE_history - table of SSE record at every iteration
  #   iter - total number of iterations at stopping/convergence

  # TODO

  def euclidian_distance(arr1,arr2):
    distance = (arr1 - arr2)**(2)
    #print(arr1, "-", arr2)
    mySum = 0
    #print('arr1:', arr1)
    #print('arr2', arr2)
    #print('distance:', distance)
    
    for i in distance:
      mySum += i
    
    mySum = mySum**(1/2)
       
       
    return mySum



  it = 0
  SSE_history = []
  cluster_list = []
  centroids = []
  data_labels = [0] * len(data)

  for i in range(K):              #For each cluster...
    cluster_list.append([0,0,0,0]) #<cluster position, cluster sum, point count, sum of l2 norm>

  cluster_distances = [] #Temporary list to store point distances from clusters

  #Setting Cluster to random Point
  for i in range(len(cluster_list)):
    rand_data_pt_index = random.randint(0, len(data) - 1) #random nth data pt
    cluster_list[i][0] = data[rand_data_pt_index] #Sets a random position of a cluster

  while it <= max_iter:
    #implment loss function break out of loop later
    # if it > 0:
    #   if np.absolute(SSE_history[it] - SSE_history[it-1])/SSE_history[it-1] <= tol:
    #     break
    
    for i in range(len(data)): #Assigning each point to a cluster

      for j in range(len(cluster_list)):
        cluster_distance = (euclidian_distance(data[i], cluster_list[j][0]), j) 
        #print("cluster_distance_tuple:",cluster_distance)
        #((data[i]**2 + cluster_list[j][0]**2)**(1/2), j) #<distance, cluster number>
        #print("Cluster Distance:",cluster_distance)
        #print("  Cluster Distance Calculation =",data[i]**2, ' -', cluster_list[j][0]**2 )
        cluster_distances.append(cluster_distance)
        #print("Cluster Distances:", cluster_distances)
      
      temp_min = cluster_distances[0] #intialization for minimum
      #print("temp_min:", temp_min)
      for j in cluster_distances: #finding which cluster has the smallest distance
        #print("j[0]: ",j[0])
        #print("j:", j)
        if j[0] < temp_min[0]:
          temp_min = j
      
      #print('temp_min:', temp_min)
      cluster_list[temp_min[1]][1] += data[i] #adding to the running sum of cluster
      cluster_list[temp_min[1]][2] += 1 #updates cluster's point count
      cluster_list[temp_min[1]][3] += temp_min[0] #should add to the l2 norm (SSE Loss)

      cluster_distance = 0
      cluster_distances.clear()
      data_labels[i] = temp_min[1] #assigns cluster number to label

    SSE_history_element = 0
    for i in range(len(cluster_list)):

      if cluster_list[i][2] != 0:
        cluster_list[i][0] = cluster_list[i][1] / cluster_list[i][2]  #calculating the new position (mean) of cluster 
        #print("am i paused")
      SSE_history_element += cluster_list[i][3] 
      
      cluster_list[i][1] = 0 #resets cluster's sum
      cluster_list[i][2] = 0 #resets cluster's number of points belonging
      cluster_list[i][3] = 0 #resets clusters SSE loss 

    SSE_history.append(SSE_history_element)
    #print('i am not paused')
    if it > 0:
     # print("sse history size:", len(SSE_history))
      if np.absolute(SSE_history[it] - SSE_history[it-1])/SSE_history[it-1] <= tol:
        break 
    
    it += 1

    #print("iters:", it)
  for i in range(len(cluster_list)):
    centroids.append(cluster_list[i][0])

  it += 1

  return data_labels, centroids, SSE_history, it

"""### Test K-means on simulated data with different values of K [3 pts]

Let us create synthetic data with `num_clusters` clusters in 2-dimensional space and apply K-means clustering. 

You should try with different values of `num_clusters` and `K'. 


"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
sns.set()

from sklearn.datasets import make_blobs

num_clusters = 4
X, y_true = make_blobs(n_samples=400,centers=num_clusters,cluster_std=5,random_state=10)
#X, y_true = make_blobs(n_samples=400,centers=num_clusters,cluster_std=3,random_state=5)

plt.scatter(X[:, 0], X[:, 1], c=y_true, s=40, cmap='viridis')
ax = plt.gca()
ax.axis('equal')

"""Let us now run K-means using different values of K.

Try `K = [num_clusters-1, num_clusters, num_clusters+1]`
"""

# TODO 
# Run K-means for different values of K. 

for K in [num_clusters-1,num_clusters,num_clusters+1]: #tests K = 3,4,5
  labels, centroids, SSE_history, iters = kmeans_clustering(X,K)

  plt.scatter(X[:, 0], X[:, 1], c=labels, s=40, cmap='viridis')
  ax = plt.gca()
  ax.axis('equal')
  plt.show()
  # TODO 
  #plot each cluster a different color
#Z, Q = make_blobs(n_samples=len(X), centers = K)


   
  # TODO 
  # plot SSE over different iterations 
  x_axis_iters = []
  for i in range(iters):
    x_axis_iters.append(i)

  #print(x_axis_iters)
  #print(SSE_history)
  
  plt.plot(x_axis_iters, SSE_history)
  plt.show()

"""### Answer the following questions [4 pts]

- Q: What happens to SSE at every iteration of K-means algorithm (did it increase/decrease)? [1 pt]

> A: At every iteration of the K-means, the SSE decreased. 


- Q: How does the SSE change as you increase K ? [1 pt]

> A:  As we increase K, the SSE decreases.

- Q: What happens if you increase the `cluster_std` of blobs above to `cluster_std=5` ? [2 pts] 
```
X, y_true = make_blobs(n_samples=400,centers=num_clusters,cluster_std=3,random_state=10)
```

> A: When increasing the cluster standard deviation from 3 to 5, the spread between the points becomes a lot greater. When we run K means clustering with higher standard deviations, the boundries between the clusters are more defined due to greater spread.

### Color segmentation/quantization [10 pts]

Now we will use K-means to perform segmentation/quantization on a color image. 

Each pixel in a given image will be a feature vector containing 3 color components (RGB). We will first group all colors into K clusters, which will provide us a color palatte. Then we will replace the color in every pixel with one of the colors in the color palatte (that is the centroid of the cluster in which RGB value of a pixel falls). 

We will use K-means script from previous step to segment your image into K clusters. To create a "quantized" output image, replace every pixel in your image with the center of the cluster assigned to it. Report your results for K= $\{2,4, 8, 16, 32, 64\}$ clusters.

__You will need a colorful selfie__

Take a _selfie_ of yourself with a background that has different colors from your skin and clothing. 

Let us say you name the image `selfie.jpg`
"""

# load and display an image with Matplotlib
from matplotlib import image

# load image as pixel array
img = image.imread('imageSegmentationSelfie.jpg')
# summarize shape of the pixel array
print(img.dtype)
print(img.shape)

# You can debug your code using some other image, 
# but you must use your selfie for final results
# from sklearn.datasets import load_sample_image
# # load the picture 
# img = load_sample_image('china.jpg')

h, w, c = img.shape
unique_colors = np.unique(np.reshape(img,(h*w,c)),axis = 0)

plt.figure()
plt.clf()
ax = plt.axes([0, 0, 1, 1])
# plt.axis('off')
plt.grid(False)
plt.title('Original image with {0:d} unique colors.'.format(unique_colors.shape[0]))
plt.imshow(img)
plt.show()

"""Before performing the clustering, we will process the image data. 

1. You can crop or resize your image to a small size if the image is large. An image close to $100\times 100$ pixels will be sufficient for this experiment. 

2. Convert 8 bit integers to floats by dividing every pixel by 255 so that we can perform floating point operations and plot the arrays as images using plt.imshow that works well on float data in the range [0-1].


"""

# Resize image to speed things up

# Improting Image class from PIL module 
from PIL import Image 

# use resize function if needed 

# TODO (if necessary)

img = Image.open('imageSegmentationSelfie.jpg')
img_size = img.size

# modify the size 
new_size = np.asarray(img_size)/4
img = img.resize(new_size.astype(int))
img = np.asarray(img)
plt.imshow(img)
plt.grid(False)

# Preprocessing the data for clustering
# convert to float64 in range [0,1]
if np.max(img) > 1:
  img = np.array(img, dtype=np.float64) / 255
plt.imshow(img)
plt.grid(False)

# Load Image and transform to a 2D numpy array.
h, w, c = original_shape = tuple(img.shape)
assert c == 3
data = np.reshape(img, (w * h, c))

plt.figure()
plt.scatter(data[:,0],data[:,1],data[:,2]) # x 3 because RGB values are truplets

"""Now we will perform two steps. 

1. K-means clustering for different values of K using the `kmeans_clustering` function above. 

2. Replace all the colors in the original image with the centroids of the cluster each of them belong to. This will give us a "segmented/quantized image" for different values of K; let us denote each image as `img_seg`. 

Plot the original image and the `img_seg` for `K = 2, 4, 8, 16, 32, 64.` 
"""

# TODO
# For K = 2, 4, 8, 16, 32, 64 
  # Perform K-means clustering for different values of K on RGB pixels; 
  # this will give you K RGB values as centroids of K clusters

  # Create a quantized image based on your cluster assignment
  # Plot original and quantized images 

# print('data shape', np.shape(data))
# print('data:', data)
# print('\n')
plt.grid(False)
plt.imshow(img)
for K in [2, 4, 8, 16, 32, 64]:
  labels, centroids, SSE_history, iters = kmeans_clustering(data,K,max_iter=100,tol=pow(10,-3))  




  img_seg = []
  for i in range(len(labels)):
    img_seg.append(centroids[labels[i]])
    labels[i] = centroids[labels[i]]

  img_seg = np.array(img_seg)
  #print(np.shape(img_seg))
  #data_to_plot = np.reshape(img_seg, (w * h, c))

  img_seg = img_seg.reshape(np.shape(img))
  plt.figure()

  plt.scatter(data[:,0],data[:,1],data[:,2], color=labels)
  plt.show()
  #print('img_seg shape:', np.shape(img_seg))
  plt.grid(False)
  plt.imshow(img_seg)

  plt.show()

"""### Answer the following questions [3 pts]

- Q: How many unique colors you have in the quantized image for K=2? [1 pt]

> A: For K = 2 there are 2 colors

- Q: How is the quality of "quantized image" affected as you increase K? [1 pt]

> A: As you increase the value of K, the quality of the image gets better because there are more and more colors

- Q: What value of K provides you best quality for the "quantized image"? [1 pt]

> A: The value of K that provides the best quality is 64

## Question 2. Eigen Faces via Principal Component Analysis [15 pts]

In this question, we will 

- Compute the PCA for a simple data set using SVD.
- Visualize the PCA for images

__Load dataset__

We will use a dataset of cropped face images called LFW ("Labeled Faces in the Wild").

This face dataset was taken from news articles about 10 years ago.  The full data set has thousands of faces, but we will use a small subset here.  Since this data set is widely-used, it is installed as part of the `sklearn`. 

We first download the data.  This is large and can take several minutes.
"""

from sklearn.datasets import fetch_lfw_people
lfw_people = fetch_lfw_people(min_faces_per_person=70, resize=0.4)

"""Once the data is loaded, we can get see the dimensions"""

# Get images
n_samples, h, w = lfw_people.images.shape
npix = h*w

# Data in 2D form
X = np.transpose(lfw_people.data) 

# NOTE: In many of the libraries the data is stored as N x d array, 
# where N is the number of training samples and d is the length of each data vector
# Since we use a different notation in the class, I will stick with that. 

# Data array has dimensions 1850 x 1288 -- (pixels) x (faces)
# Each data vector is stored as a column in the matrix. 

# Labels of images 
y = lfw_people.target
target_names = lfw_people.target_names 

print("Image size     = {0:d} x {1:d} = {2:d} pixels".format(h,w,npix))
print("Number faces   = {0:d}".format(n_samples))

"""__Plotting the Faces__

We will plot a few faces to look at the dataset.
"""

def plt_face(x):
    h = 50
    w = 37
    plt.imshow(x.reshape((h, w)), cmap=plt.cm.gray)
    plt.xticks([])
    plt.yticks([])
    
I = np.random.permutation(n_samples)
plt.figure(figsize=(10,20))

T = 4
for i in range(T):    
    ind = I[i]
    plt.subplot(1,T,i+1)
    plt_face(X[:,ind])
    plt.title(target_names[y[ind]])

"""### Computing the PCA via SVD 
To compute principal components (PCs), you will perform two steps. 

1. Subtract the mean from the data set.
2. Compute the singular value decomposition (SVD)

Suppose $X$ is the $D\times N$ data matrix. ($D$ is the number of pixels in each image, $N$ is the number of images.)

You will first remove mean column from all the columns (because data vectors are stored as columns). 

``` Xs = X - np.mean(X,1,keepdims = True) ``` 

Then we will compute an SVD of mean-substracted data as 

``` U_, S_, Vt_ = np.linalg.svd(Xs, full_matrices=False)```

Note that in python the SVD returns a list of singular values and `V.T` instead of `V`. 
The `full_matrices` option gives the *economy* SVD

"""

# Compute SVD 

# subtract mean from the dataset 
# since data vectors are stored as columns, the mean of columns should be zero

npix = h*w
Xmean = np.mean(X,1,keepdims=True)
Xs = X - Xmean

U_,S_,Vt_ = np.linalg.svd(Xs, full_matrices=False)

# We can plot the singular values

plt.figure()
plt.plot(S_)
plt.ylabel('singular values')
plt.xlabel('index')

"""Next we will approximate the data using $r$ principal components/factors as

    X ~= Xhat = Xmean + U.dot(Z) 
    
The terms

*  `U` are the top-$r$ principal components/factors from `U_` computed from data.
*  `Z` are the coefficients of data samples (or projections onto principal components). Note that we can compute $Z = U^T X$.  

For instance, we can compute $Z$ for $r = 20$ and the approximate data as
"""

# Compute Z for r = 20 
N = X.shape[1]
Z = U_.T.dot(Xs) 

r = 20
Xest = Xmean + U_[:,:r].dot(Z[:r,:])

# plot some sample images
T = 3; 
inds = np.random.permutation(n_samples)
inds = inds[:T]
print(inds)

# plot approximated faces
print('Approximated faces (above)')
for i in range(T):
  plt.subplot(1,T,i+1)
  # plt.imshow(np.reshape(Xest[:,inds[i]],(h,w)))
  plt_face(Xest[:,inds[i]])
  plt.grid(False)
  plt.axis('off')
  plt.title(target_names[y[inds[i]]])
  if i == 0:
    plt.ylabel('Approximation')

plt.show()

# plot original faces 
for i in range(T):
  plt.subplot(1,T,i+1)
  # plt.imshow(np.reshape(X[:,inds[i]],(h,w)))
  plt_face(X[:,inds[i]])
  plt.grid(False)
  plt.axis('off')
  # plt.title(target_names[y[inds[i]]])
  if i == 0:  
    plt.ylabel('Original')

plt.show()
plt.tight_layout()

print('Original faces (below)')

"""### Plotting Approximated Images from PCs [10 pts]

We will now plot the images approximated using different values of $r$. 

Select 3 images at random from the dataset and find its approximation using top  $r$ factors for $r = \{0, 5, 10, 50, 100, 200\}$. 
"""

# TODO 
  # select 3 images
  # select r = [0, 5, 10, 50, 100, 200]
  # find approximation of selected images and plot them

T = 3                             # number of faces to plot
r_list = [0, 5,10,50,100, 200]    # number of SVD approximations
nr = len(r_list)

# TODO 
# Select random faces 
inds = np.random.permutation(n_samples)
inds = inds[:T] 

# TODO 
# Loop over figures
# compute approximation with r factors 
iplt = 0
for ind in inds:
    for r in r_list:
        plt.subplot(T,nr+1,iplt+1)
        
        # Reconstruct with SVD
        # TODO  
        # Xhati = ???  # estimate of image in the r-dim subspace
        # U_,S_,Vt_ = np.linalg.svd(Xs, full_matrices=False)
        # N = X.shape[1]
        # Z = U_.T.dot(Xs) 

        Xhati = Xmean + U_[:,:r].dot(Z[:r,:])

        plt_face(Xhati[:,inds[0]])
        #plt.title('r={0:d}'.format(d))
        iplt += 1
    
    # Plot the true face
    plt.subplot(T,nr+1,iplt+1)
    plt_face(X[:,ind])
    plt.axis('off')
    plt.title('Full')
    iplt += 1

"""Finally, pick 10 top PCs and plot them as images. """

# TODO 
  # Plot first 10 PCs (i.e., columns in U_) as images
for imageIndex in range(0,10):
  plt.subplot(5,5,imageIndex+1)
  plt_face(U_[:,imageIndex])

"""### Best and Worst Approximation [5 pts]

You can convince yourself that the $\ell_2$ norm of the approximation error of any data vector is same as the $\ell_2$ norm of the unused coefficients. 

Using this property, find the best and worst 5 images in the dataset that give smallest and largest error in approximation with top 200 PCs? Plot them. 
"""

# We can compute the L2 norm of the approximation error for each image as 
# the L2 norm of the unused coefficients

r = 200
Xest = Xmean + U_[:,:r].dot(Z[:r,:])

est_error = np.sum(pow(Xest-X,2),axis=0)
plt.figure()
plt.plot(est_error)
plt.xlabel('image index')
plt.ylabel('approximation error')


Z_residual = np.sum(pow(Z[r:,:],2),axis=0)
plt.figure()
plt.plot(Z_residual)
plt.xlabel('image index')
plt.ylabel('Z residual')

# TODO
# Plot the "Worst/Hardest" images to reconstruct with 200 PCs

# Sort error/residual in decreasing order and pick top 5 indices 

# your code goes here

# TODO
# Plot the "Best/Easiest" images to reconstruct with 200 PCs

# Sort error/residual in increasing order and pick top 5 indices 

# your code goes here

"""---
## Submission instructions
1. Download this Colab to ipynb, and convert it to PDF. Follow similar steps as [here](https://stackoverflow.com/questions/53460051/convert-ipynb-notebook-to-html-in-google-colab) but convert to PDF.
 - Download your .ipynb file. You can do it using only Google Colab. `File` -> `Download` -> `Download .ipynb`
 - Reupload it so Colab can see it. Click on the `Files` icon on the far left to expand the side bar. You can directly drag the downloaded .ipynb file to the area. Or click `Upload to session storage` icon and then select & upload your .ipynb file.
 - Conversion using %%shell. 
 ```
!sudo apt-get update
!sudo apt-get install texlive-xetex texlive-fonts-recommended texlive-generic-recommended
!jupyter nbconvert --log-level CRITICAL --to pdf name_of_hw.ipynb
  ```
 - Your PDF file is ready. Click 3 dots and `Download`.


  

2. Upload the PDF to Gradescope, select the correct pdf pages for each question. **Important!**

3. Upload the ipynb file to Gradescope


Notice:
In case of errors in conversion, please check your LaTeX and debug. In Markdown, when you write in LaTeX math mode, do not leave any leading and trailing whitespaces inside the dollar signs ($). For example, write `(dollarSign)\mathbf(dollarSign)(dollarSign)` instead of `(dollarSign)(space)\mathbf{w}(dollarSign)`. Otherwise, nbconvert will throw an error and the generated pdf will be incomplete. [This is a bug of nbconvert.](https://tex.stackexchange.com/questions/367176/jupyter-notebook-latex-conversion-fails-escaped-and-other-symbols)

"""

!sudo apt-get update
!sudo apt-get install texlive-xetex texlive-fonts-recommended texlive-generic-recommended

!jupyter nbconvert --log-level CRITICAL --to pdf fall2022_hw4.ipynb # make sure the ipynb name is correct

